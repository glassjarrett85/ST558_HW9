[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ST558 Assignment 09",
    "section": "",
    "text": "Note: This is assignment 09, but the first part of it will be brought in from assignment 08.\nSkip to the new content for Assignment 09"
  },
  {
    "objectID": "index.html#assignment-9",
    "href": "index.html#assignment-9",
    "title": "ST558 Assignment 09",
    "section": "To Do:",
    "text": "To Do:\n\nAdding to this documentation, the first recipe from above (using no quadratics or interactions) should be modeled using:\n\na (tuned) LASSO model\na (tuned) Regression Tree model\na (tuned) Bagged Tree model\na (tuned) Random Forest model\n\nEach model should be fit and tuned on the training set, and the best model from each family of models should be fit to the training data set and used to see how well it predicts the test set.\n\nTaking from above, the first recipe (recipe.1) will be modeled with the various modalities outlined.\nThe first modeled procedure will be the LASSO method.\n\n# First method: LASSO. Establish with recipe.1 and the GLMNET engine.\nLASSO_wkfl &lt;- workflow() |&gt;\n  add_recipe(recipe.1) |&gt;\n  add_model(linear_reg(penalty=tune(), mixture=1) |&gt; \n              set_engine(\"glmnet\"))\n\n# Generate the Grid of model fits on the 10-fold CV variable.\nLASSO_grid &lt;- LASSO_wkfl |&gt; \n  tune_grid(resamples = bike_train_10fold,\n            grid=grid_regular(penalty(), levels=200))\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x12\n\n\nThere were issues with some computations   A: x15\n\n\nThere were issues with some computations   A: x18\n\n\nThere were issues with some computations   A: x21\n\n\nThere were issues with some computations   A: x24\n\n\nThere were issues with some computations   A: x27\n\n\nThere were issues with some computations   A: x30\nThere were issues with some computations   A: x30\n\n\n\n\nLASSO_grid\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [400 × 5]&gt; &lt;tibble [3 × 4]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x30: A correlation computation is required, but `estimate` is constant...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n# The lowest RMSE is\nLASSO_lowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric=\"rmse\")\nLASSO_lowest_rmse\n\n# A tibble: 1 × 2\n       penalty .config          \n         &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001 pre0_mod001_post0\n\n# Fitting this best-tuned model on the training set,\nLASSO_final &lt;- LASSO_wkfl |&gt;\n  finalize_workflow(LASSO_lowest_rmse) |&gt;\n  fit(bike_train)\n\n# Running the LASSO model on the Test set\nLASSO_test_metrics &lt;- LASSO_wkfl |&gt;\n  finalize_workflow(LASSO_lowest_rmse) |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics()\nLASSO_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       0.407 pre0_mod0_post0\n2 rsq     standard       0.841 pre0_mod0_post0\n\n\nThe second model to be evaluated is the Regression Tree model.\n\n# Establish the Regression Tree model using recipe.1:\nlibrary(tree)\ntree_wkfl &lt;- workflow() |&gt;\n  add_recipe(recipe.1) |&gt;\n  add_model(decision_tree(tree_depth = tune(),\n                          min_n=20,\n                          cost_complexity = tune()) |&gt;\n              set_engine(\"rpart\") |&gt;\n              set_mode(\"regression\"))\n\n# Creation of the tuning grid\ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = c(10, 5))\n\n# Tune these by fitting to the CV-Folds\ntree_fits &lt;- tree_wkfl |&gt;\n  tune_grid(resamples=bike_train_10fold, grid=tree_grid)\ntree_fits |&gt; collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          1 rmse    standard   0.684    10  0.0408 pre0_mod01…\n 2    0.0000000001          1 rsq     standard   0.537    10  0.0458 pre0_mod01…\n 3    0.0000000001          4 rmse    standard   0.463    10  0.0240 pre0_mod02…\n 4    0.0000000001          4 rsq     standard   0.785    10  0.0201 pre0_mod02…\n 5    0.0000000001          8 rmse    standard   0.403    10  0.0207 pre0_mod03…\n 6    0.0000000001          8 rsq     standard   0.841    10  0.0148 pre0_mod03…\n 7    0.0000000001         11 rmse    standard   0.403    10  0.0207 pre0_mod04…\n 8    0.0000000001         11 rsq     standard   0.841    10  0.0148 pre0_mod04…\n 9    0.0000000001         15 rmse    standard   0.403    10  0.0207 pre0_mod05…\n10    0.0000000001         15 rsq     standard   0.841    10  0.0148 pre0_mod05…\n# ℹ 90 more rows\n\n# The optimum tuning parameter\ntree_best &lt;- select_best(tree_fits, metric=\"rmse\")\ntree_best\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1    0.0000000001          8 pre0_mod03_post0\n\n# Fit the final model on the entire training set\ntree_final_fit &lt;- tree_wkfl |&gt;\n  finalize_workflow(tree_best) |&gt;\n  last_fit(bike_split)\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       0.383 pre0_mod0_post0\n2 rsq     standard       0.858 pre0_mod0_post0\n\n\nThe third model is the Bagged Tree Model.\n\nlibrary(baguette)\nbag_wkfl &lt;- workflow() |&gt;\n  add_recipe(recipe.1) |&gt;\n  add_model(bag_tree(tree_depth=5, min_n=10, cost_complexity=tune()) |&gt;\n              set_engine(\"rpart\") |&gt;\n              set_mode(\"regression\"))\n\n# Fit this workflow to our 10-fold CV fits\nbag_fit &lt;- bag_wkfl |&gt;\n  tune_grid(resamples=bike_train_10fold,\n            grid=grid_regular(cost_complexity(), levels=15),\n            metrics=metric_set(rmse, rsq, mae))\n\n# The best tuning parameter from this 10-fold CV fit\nbag_best &lt;- select_best(bag_fit, metric=\"rmse\")\nbag_best\n\n# A tibble: 1 × 2\n  cost_complexity .config         \n            &lt;dbl&gt; &lt;chr&gt;           \n1        0.000268 pre0_mod11_post0\n\n# Fit the entire training set on this tuning parameter\nbag_final_fit &lt;- bag_wkfl |&gt;\n  finalize_workflow(bag_best) |&gt;\n  last_fit(bike_split, metrics=metric_set(rmse, rsq, mae))\n\n# The final metrics for this model\nbag_final_fit |&gt; collect_metrics()\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       0.308 pre0_mod0_post0\n2 rsq     standard       0.907 pre0_mod0_post0\n3 mae     standard       0.240 pre0_mod0_post0\n\n\nFinally, a tuned random forest.\n\nlibrary(ranger)\nrf_wkfl &lt;- workflow() |&gt;\n  add_recipe(recipe.1) |&gt;\n  add_model(rand_forest(mtry = tune(), trees=500) |&gt;\n              set_engine(\"ranger\", importance=\"permutation\") |&gt;\n              set_mode(\"regression\"))\n\n# Fit the model to the CV folds\nrf_fit &lt;- rf_wkfl |&gt;\n  tune_grid(resamples = bike_train_10fold,\n            grid=7,\n            metrics=metric_set(rmse, rsq, mae))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n# The metrics across the folds:\nrf_fit |&gt; \n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config        \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1    15 rmse    standard   0.284    10  0.0125 pre0_mod7_post0\n2    13 rmse    standard   0.287    10  0.0120 pre0_mod6_post0\n3    10 rmse    standard   0.290    10  0.0114 pre0_mod5_post0\n4     8 rmse    standard   0.293    10  0.0117 pre0_mod4_post0\n5     5 rmse    standard   0.300    10  0.0116 pre0_mod3_post0\n6     3 rmse    standard   0.319    10  0.0121 pre0_mod2_post0\n7     1 rmse    standard   0.496    10  0.0214 pre0_mod1_post0\n\n# Select the best entry\nbest_mtry &lt;- rf_fit |&gt;\n  select_best(metric=\"rmse\") |&gt;\n  pull(mtry)\n\n# Get the best tuning parameter and refit on the entire training set\nrf_final_fit &lt;- rf_wkfl |&gt;\n  finalize_workflow(select_best(rf_fit, metric=\"rmse\")) |&gt;\n  last_fit(bike_split, metrics=metric_set(rmse, rsq, mae))\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       0.284 pre0_mod0_post0\n2 rsq     standard       0.921 pre0_mod0_post0\n3 mae     standard       0.223 pre0_mod0_post0\n\n\n\n\n\nAll final models (including best MLR from Assignment 08) should be compared using both RMSE and MAE.\n\nFor LASSO and MLR, report final coefficient table.\nFor the regression tree model, give a plot of the final fit.\nFor the bagged tree and random forest models, produce a variable importance plot.\n\nFind the overall best model, and fit this to the entire data set.\nThe final coefficient tables, utilizing the same coefficient table from Assignment 08 and replicating for the LASSO method:\n\n# MLR, From assignment 08\ncoef_table\n\n# A tibble: 44 × 5\n   term                         estimate std.error statistic  p.value\n   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                  19.8     15.9          1.24  2.15e- 1\n 2 date                         -0.00113  0.000894    -1.27  2.06e- 1\n 3 rainfall_mm_sum              -0.709    0.230       -3.08  2.35e- 3\n 4 snowfall_cm_sum              -0.0150   0.0187      -0.800 4.25e- 1\n 5 temperature_c_mean            0.377    0.372        1.01  3.12e- 1\n 6 humidity_percent_mean        -0.112    0.126       -0.890 3.74e- 1\n 7 wind_speed_m_s_mean          -0.0495   0.0185      -2.68  7.97e- 3\n 8 visibility_10m_mean           0.0447   0.0238       1.88  6.14e- 2\n 9 dew_point_temperature_c_mean  0.195    0.423        0.461 6.46e- 1\n10 solar_radiation_mj_m2_mean    0.234    0.0327       7.14  1.18e-11\n# ℹ 34 more rows\n\n# The LASSO method\nLASSO_wkfl |&gt;\n  finalize_workflow(LASSO_lowest_rmse) |&gt;\n  fit(bike_train) |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 16 × 3\n   term                          estimate      penalty\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)                  -3.30e-16 0.0000000001\n 2 rainfall_mm_sum              -2.11e- 1 0.0000000001\n 3 snowfall_cm_sum              -3.21e- 2 0.0000000001\n 4 temperature_c_mean            0        0.0000000001\n 5 humidity_percent_mean        -1.13e- 1 0.0000000001\n 6 wind_speed_m_s_mean          -7.93e- 2 0.0000000001\n 7 visibility_10m_mean           0        0.0000000001\n 8 dew_point_temperature_c_mean  4.04e- 1 0.0000000001\n 9 solar_radiation_mj_m2_mean    3.75e- 1 0.0000000001\n10 rainfall_mm_mean              0        0.0000000001\n11 snowfall_cm_mean             -6.06e- 4 0.0000000001\n12 seasons_Spring               -2.32e- 1 0.0000000001\n13 seasons_Summer               -1.22e- 1 0.0000000001\n14 seasons_Winter               -3.79e- 1 0.0000000001\n15 holiday_No.Holiday            8.74e- 2 0.0000000001\n16 daytype_Weekend              -1.07e- 1 0.0000000001\n\n# Regression Tree Model - plot of the final fit\ntree_final_model &lt;- extract_workflow(tree_final_fit)\ntree_final_model |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint=FALSE)\n\n\n\n\n\n\n\n# Next, the variable importance plot for the bagged tree model:\nbag_final_model &lt;- extract_fit_engine(bag_final_fit)\nbag_final_model$imp |&gt;\n  mutate(term=factor(term, levels=term)) |&gt;\n  ggplot(aes(x=term, y=value)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip()"
  }
]